Metadata-Version: 2.4
Name: tabc-scrape
Version: 1.0.0
Summary: A comprehensive CLI tool for collecting, enriching, and analyzing Texas restaurant data from the Comptroller API
Home-page: https://github.com/tabc-scrape/tabc-scrape
Author: TABC Scraper Team
Author-email: info@tabc-scrape.com
Keywords: texas,restaurant,data-scraping,api,comptroller,data-enrichment,cli,database,validation
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Information Analysis
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Database
Classifier: Topic :: Internet :: WWW/HTTP :: Indexing/Search
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: pandas>=1.5.0
Requires-Dist: numpy>=1.21.0
Requires-Dist: requests>=2.28.0
Requires-Dist: beautifulsoup4>=4.11.0
Requires-Dist: selenium>=4.5.0
Requires-Dist: fake-useragent>=1.1.0
Requires-Dist: geopy>=2.3.0
Requires-Dist: folium>=0.13.0
Requires-Dist: censusdata>=1.15
Requires-Dist: sqlalchemy>=1.4.0
Requires-Dist: alembic>=1.8.0
Requires-Dist: psycopg2-binary>=2.9.0
Requires-Dist: alembic-utils>=0.8.0
Requires-Dist: requests-ratelimiter>=0.3.0
Requires-Dist: pydantic>=1.10.0
Requires-Dist: great-expectations>=0.15.0
Requires-Dist: structlog>=22.0.0
Requires-Dist: selenium>=4.5.0
Requires-Dist: fake-useragent>=1.1.0
Requires-Dist: requests-html>=0.10.0
Requires-Dist: transformers>=4.21.0
Requires-Dist: torch>=1.12.0
Requires-Dist: scikit-learn>=1.1.0
Requires-Dist: nltk>=3.7
Requires-Dist: spacy>=3.4.0
Requires-Dist: pandas>=1.5.0
Requires-Dist: numpy>=1.21.0
Requires-Dist: click>=8.0.0
Requires-Dist: pytest>=7.0.0
Requires-Dist: pytest-asyncio>=0.21.0
Requires-Dist: black>=22.0.0
Requires-Dist: flake8>=5.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: flake8>=5.0.0; extra == "dev"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# TABC Restaurant Data Scraper

A comprehensive CLI tool for collecting, enriching, and analyzing Texas restaurant data from the Comptroller API.

## Features

- **ğŸ“Š Data Collection**: Fetch restaurant data from the Texas Comptroller API
- **ğŸ”¬ Data Enrichment**: Enhance data with concept classification, population analysis, and square footage information
- **ğŸ“¤ Data Export**: Export enriched data to JSON or CSV formats
- **ğŸ” Data Validation**: Comprehensive data quality validation and reporting
- **ğŸ“ˆ Status Monitoring**: Real-time system status and statistics

## Installation

### From Source (Recommended for Development)

```bash
# Clone the repository
git clone <repository-url>
cd tabc-scrape

# Install in development mode
pip install -e .

# Or install with all dependencies
pip install -r requirements.txt
```

### System-wide Installation

```bash
# Install the package system-wide
pip install .

# The CLI will be available as 'tabc-scrape' command
```

## Quick Start

### 1. Fetch Restaurant Data

```bash
# Fetch all restaurant data (be respectful with API limits)
tabc-scrape fetch

# Fetch with custom batch size
tabc-scrape fetch --batch-size 500

# Fetch limited number of records for testing
tabc-scrape fetch --limit 100
```

### 2. Enrich the Data

```bash
# Run full enrichment pipeline
tabc-scrape enrich

# Run with custom batch size
tabc-scrape enrich --batch-size 5

# Skip specific enrichment steps
tabc-scrape enrich --skip-square-footage --skip-population-analysis

# Process only a subset of restaurants
tabc-scrape enrich --limit 50
```

### 3. Export Enriched Data

```bash
# Export to JSON (default)
tabc-scrape export

# Export to CSV
tabc-scrape export --format csv

# Export to specific file
tabc-scrape export --output ./data/my_restaurants.json
```

### 4. Validate Data Quality

```bash
# Basic validation
tabc-scrape validate

# Detailed validation with top issues
tabc-scrape validate --detailed

# Save validation report
tabc-scrape validate --output-report ./reports/validation.json

# Validate specific file
tabc-scrape validate --input-file ./data/restaurants.json
```

### 5. Check System Status

```bash
# Show current status and statistics
tabc-scrape status
```

## Command Reference

### Global Options

- `--help` or `-h`: Show help for any command
- `--version`: Show version information

### `tabc-scrape fetch`

Fetch restaurant data from Texas Comptroller API.

**Options:**
- `-l, --limit INTEGER`: Maximum number of restaurants to fetch
- `-b, --batch-size INTEGER`: Batch size for API requests (default: 1000)
- `--database-url TEXT`: Database URL override

**Examples:**
```bash
tabc-scrape fetch --limit 1000 --batch-size 500
```

### `tabc-scrape enrich`

Run data enrichment pipeline.

**Options:**
- `-l, --limit INTEGER`: Maximum number of restaurants to enrich
- `-b, --batch-size INTEGER`: Batch size for enrichment (default: 10)
- `--database-url TEXT`: Database URL override
- `--skip-square-footage`: Skip square footage scraping
- `--skip-concept-classification`: Skip concept classification
- `--skip-population-analysis`: Skip population analysis

**Examples:**
```bash
tabc-scrape enrich --batch-size 5 --skip-square-footage
```

### `tabc-scrape export`

Export enriched restaurant data.

**Options:**
- `-f, --format [json|csv]`: Export format (default: json)
- `-o, --output TEXT`: Output file path
- `--database-url TEXT`: Database URL override
- `--enriched-only`: Export only enriched restaurants

**Examples:**
```bash
tabc-scrape export --format csv --output ./exports/restaurants.csv
```

### `tabc-scrape validate`

Validate data quality.

**Options:**
- `-i, --input-file TEXT`: Input file to validate (optional)
- `-o, --output-report TEXT`: Output validation report file
- `--database-url TEXT`: Database URL override
- `--detailed`: Show detailed validation results

**Examples:**
```bash
tabc-scrape validate --detailed --output-report ./reports/quality.json
```

### `tabc-scrape status`

Show current system status and statistics.

**Examples:**
```bash
tabc-scrape status
```

## Configuration

### Environment Variables

The application can be configured using environment variables:

- `TABC_API_URL`: Override the default Texas Comptroller API URL
- `TABC_DB_URL`: Override the default database URL
- `TABC_SCRAPING_DELAY`: Set delay between scraping requests (seconds)

### Database Configuration

By default, the application uses SQLite (`sqlite:///tabc_restaurants.db`). You can override this by:

1. Setting the `TABC_DB_URL` environment variable
2. Using the `--database-url` option with commands

**Example:**
```bash
export TABC_DB_URL="postgresql://user:password@localhost/tabc_data"
tabc-scrape fetch
```

## Data Pipeline Overview

1. **Fetch**: Collect raw restaurant data from Texas Comptroller API
2. **Store**: Save raw data to local database
3. **Enrich**: Enhance data with:
   - Concept classification (restaurant type/category)
   - Population analysis (demographics within radius)
   - Square footage information (scraped from web)
4. **Validate**: Check data quality and generate reports
5. **Export**: Export enriched data for analysis

## Output Files

- **Database**: `tabc_restaurants.db` (SQLite by default)
- **Logs**: `tabc_scrape.log`
- **Exports**: `data/enriched_restaurants_export.json` (or CSV)
- **Reports**: `reports/validation.json` (if generated)

## Development

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src/tabc_scrape

# Run specific test file
pytest tests/test_api_client.py
```

### Code Quality

```bash
# Format code
black src/ tests/

# Lint code
flake8 src/ tests/

# Type checking (if mypy is installed)
mypy src/
```

### Project Structure

```
tabc-scrape/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ tabc_scrape/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ cli.py              # Main CLI entry point
â”‚       â”œâ”€â”€ config.py           # Configuration management
â”‚       â”œâ”€â”€ data/
â”‚       â”‚   â””â”€â”€ api_client.py   # Texas Comptroller API client
â”‚       â”œâ”€â”€ scraping/
â”‚       â”‚   â”œâ”€â”€ concept_classifier.py
â”‚       â”‚   â””â”€â”€ square_footage.py
â”‚       â”œâ”€â”€ storage/
â”‚       â”‚   â”œâ”€â”€ database.py
â”‚       â”‚   â”œâ”€â”€ enrichment_pipeline.py
â”‚       â”‚   â”œâ”€â”€ models.py
â”‚       â”‚   â””â”€â”€ validation_framework.py
â”‚       â””â”€â”€ utils/
â”œâ”€â”€ tests/                      # Test files
â”œâ”€â”€ data/                       # Data exports
â”œâ”€â”€ reports/                    # Validation reports
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## API Reference

### Core Classes

- `TexasComptrollerAPI`: Handles communication with Texas Comptroller API
- `DatabaseManager`: Manages database operations and data storage
- `DataEnrichmentPipeline`: Coordinates the enrichment process
- `ValidationReporter`: Generates data quality reports

### Data Models

- `RestaurantRecord`: Restaurant data structure
- `EnrichmentResult`: Results from enrichment operations
- `QualityReport`: Comprehensive data quality assessment

## Troubleshooting

### Common Issues

1. **API Rate Limiting**: The Texas Comptroller API has rate limits. Use the `--batch-size` and respect delays between requests.

2. **Database Connection Errors**: Ensure the database URL is correct and the database server is running.

3. **Missing Dependencies**: Install all requirements:
   ```bash
   pip install -r requirements.txt
   ```

4. **Scraping Issues**: Some websites may block automated requests. The scraper includes delays and user agent rotation.

### Logging

The application logs to both console and `tabc_scrape.log` file. Log levels can be controlled via the logging configuration in `cli.py`.

### Debug Mode

For more detailed logging, you can modify the logging level in the CLI module:

```python
logging.basicConfig(level=logging.DEBUG, ...)
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Run the test suite
6. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Support

For issues, questions, or contributions, please:

1. Check the existing issues on GitHub
2. Create a new issue with detailed information
3. Include error messages, configuration, and steps to reproduce

## Changelog

### Version 1.0.0

- Initial release
- Complete CLI interface with all required commands
- Data fetching from Texas Comptroller API
- Comprehensive enrichment pipeline
- Data validation and quality reporting
- Export functionality (JSON/CSV)
- System status monitoring
